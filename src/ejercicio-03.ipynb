{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24ae5583-9170-4a86-817a-45ddb1a9d857",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data Engineering Test\n",
    "# Solvex - 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d29a3588-d9d5-487b-ae12-b69609be9c02",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Resuelto por Santiago Taracena Puga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb489ff8-43f6-4ed9-9792-55bba38b8a86",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Ejercicio 3: Integración de Pandas y Spark con datos de películas y críticas\n",
    "\n",
    "Supongamos que tienes dos conjuntos de datos: uno en Pandas y otro en Spark. El conjunto de datos de Pandas es una tabla llamada \"datos_peliculas\" con información sobre películas:\n",
    "\n",
    "```\n",
    "ID,Título,Año\n",
    "1,Película1,2020\n",
    "2,Película2,2019\n",
    "3,Película3,2021\n",
    "4,Película4,2018\n",
    "```\n",
    "\n",
    "El conjunto de datos de Spark es un DataFrame llamado \"criticas\" con información sobre las críticas de películas:\n",
    "\n",
    "```\n",
    "PeliculaID,Critico,Puntuacion\n",
    "1,Critico1,4.5\n",
    "2,Critico2,3.8\n",
    "3,Critico1,4.2\n",
    "4,Critico3,4.7\n",
    "```\n",
    "\n",
    "Combina estos dos conjuntos de datos para obtener una tabla que muestre el título de la película, el año de lanzamiento y la puntuación promedio de las críticas. Asegúrate de utilizar tanto Pandas como Spark en el proceso de integración."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "570bae5a-32c7-4cca-873d-04b191c8d5dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eec92df4-d707-4407-ac0e-08832b030611",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Para la resolución del presente ejercicio, es necesario en primer lugar importar y crear el primer dataset correspondiente a Pandas. Podemos importar Pandas como lo hemos hecho anteriormente, y también utilizar la clase `DataFrame` con el objetivo de crear el primer dataset mencionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fef62f70-a075-4f00-829c-91f52499899f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Instrucción para importar Pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85dd3d0b-cb31-4c72-bf9c-8f8ce8d69a92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Película1</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Película2</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Película3</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Película4</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Película1</td>\n      <td>2020</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Película2</td>\n      <td>2019</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Película3</td>\n      <td>2021</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Película4</td>\n      <td>2018</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creación del dataset de películas\n",
    "movies_data = pd.DataFrame({\n",
    "    \"id\": [1, 2, 3, 4],\n",
    "    \"title\": [\"Película1\", \"Película2\", \"Película3\", \"Película4\"],\n",
    "    \"year\": [2020, 2019, 2021, 2018],\n",
    "})\n",
    "movies_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c1323aa-d3e7-46cf-9cf0-d47874f9fe6b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Con el primer dataset de Pandas finalizado, lo siguiente es crear el dataset de Spark. Tenemos qué importar Spark como lo hemos realizado anteriormente, y continuar el procedimiento de instanciarlo y crear un DataFrame que en lugar de Pandas sea de Spark. Podemos crear un DataFrame de Spark con la función `createDataFrame`, pasando como argumentos la data y las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8607a110-a779-40fc-9815-200af419c09d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Instrucción para importar Spark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daaf4e8b-1248-4d52-87f6-55068eac1731",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=1412980896774091#setting/sparkui/0428-234447-2z1l5d47/driver-7247733620141223775\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=1412980896774091#setting/sparkui/0428-234447-2z1l5d47/driver-7247733620141223775\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instancia para trabajar con Spark\n",
    "spark = SparkSession.builder.appName(\"PandasSpark\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63b0e305-63c0-4065-901d-dba9ebeb99f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+\n",
      "|movie_id|reviewer|score|\n",
      "+--------+--------+-----+\n",
      "|       0|Crítico1|  4.5|\n",
      "|       1|Crítico2|  3.8|\n",
      "|       2|Crítico1|  4.2|\n",
      "|       3|Crítico3|  4.7|\n",
      "+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creación del dataset de Spark\n",
    "data = [\n",
    "    (0, \"Crítico1\", 4.5),\n",
    "    (1, \"Crítico2\", 3.8),\n",
    "    (2, \"Crítico1\", 4.2),\n",
    "    (3, \"Crítico3\", 4.7),\n",
    "]\n",
    "reviews = spark.createDataFrame(data, [\"movie_id\", \"reviewer\", \"score\"])\n",
    "reviews.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6665595-7756-407d-bf4e-0f2778eff998",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Con los dos datasets, podemos hacer una integración entre los mismos dos creando un DataFrame de Spark con la función `createDataFrame` anterior. Spark tiene una particularidad muy útil, especialmente con esta función, y es que si pasamos un DataFrame de Pandas, este mismo puede ser casteado a uno de Spark sin ningún problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebd819dd-0011-497d-80bb-1ca8b24616e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----+\n",
      "| id|    title|year|\n",
      "+---+---------+----+\n",
      "|  1|Película1|2020|\n",
      "|  2|Película2|2019|\n",
      "|  3|Película3|2021|\n",
      "|  4|Película4|2018|\n",
      "+---+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Casteo del DataFrame de Pandas a Spark\n",
    "movies_data = spark.createDataFrame(movies_data)\n",
    "movies_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "835cf96b-ce2f-4d49-8160-54c4546d4ad4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Con ambos datasets en el formato que necesitamos, podemos proceder a ejecutar un join para tener los datos de las dos tablas unidos para obtener el promedio mencionado. La función `join` nos permite realizar este join, identificando la tabla a unir, la condición del join y qué tipo de join es. Básicamente estamos haciendo un ```SELECT * FROM movies INNER JOIN reviews ON movies.id = reviews.movie_id```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f51df647-3ada-4e7b-8433-7b17966a4c66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----+--------+--------+-----+\n",
      "| id|    title|year|movie_id|reviewer|score|\n",
      "+---+---------+----+--------+--------+-----+\n",
      "|  1|Película1|2020|       1|Crítico2|  3.8|\n",
      "|  2|Película2|2019|       2|Crítico1|  4.2|\n",
      "|  3|Película3|2021|       3|Crítico3|  4.7|\n",
      "+---+---------+----+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join entre las dos tablas creadas\n",
    "joined_data = movies_data.join(reviews, movies_data.id == reviews.movie_id, \"inner\")\n",
    "joined_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e649fe4-e14c-4d5f-ae4a-45b1911aff3f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Finalmente, con los dos datasets unidos, únicamente hace falta obtener el promedio de cada película agrupada por título y año. Esto podemos hacerlo con la función `groupBy`, y posteriormente utilizando la función de agregación `avg` para obtener el promedio solicitado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e6848d7-56ce-4c1a-9ce9-d5a8fd033f50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Función avg de Spark\n",
    "from pyspark.sql.functions import avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46322ce6-fb49-43aa-87cd-d44ea15e384f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+\n",
      "|    title|year|avg_score|\n",
      "+---------+----+---------+\n",
      "|Película1|2020|      3.8|\n",
      "|Película2|2019|      4.2|\n",
      "|Película3|2021|      4.7|\n",
      "+---------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tabla resultante de obtener los promedios\n",
    "result = joined_data.groupBy(\"title\", \"year\").agg(avg(\"score\").alias(\"avg_score\"))\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ejercicio-03",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
